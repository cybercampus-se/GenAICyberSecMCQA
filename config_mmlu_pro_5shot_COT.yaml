# Yamel configuration file for MMLU PRO

# Configurations of MCQA with CybSec Certifications?

control_output: true

# Dictionary with the model paths and the corresponding model names
model_paths:
  # Phi-3-medium-4k-instruct: "vllm:microsoft/Phi-3-medium-4k-instruct"
  # Phi-3-vision-128k-instruct: "vllm:microsoft/Phi-3-vision-128k-instruct"
  # Llama3.1 70B Instruct: "ollama:llama3.1:70b-instruct-fp16"
  # Llama3.1 405B Instruct: "nim:meta/llama-3.1-405b-instruct"
  # Qwen2-72B-Instruct: "ollama:qwen2:72b-instruct-fp16"
  # Mistral Large 2: "ollama:mistral-large:123b-instruct-2407-fp16"
  # OpenAI GPT-4o: "openai:gpt-4o-2024-05-13"
  # Anthropic Claude 3.5 Sonnet: "anthropic:claude-3-5-sonnet-20240620"
  # Qwen2.5-72B-Instruct: "vllm:Qwen/Qwen2.5-72B-Instruct"
  # Claude 3.5 Sonnet (new): "anthropic:claude-3-5-sonnet-20241022"
  # Llama-3.1-8B-Instruct : ${LLM_SERVER_URL}
  Llama-3.1-8B : meta-llama/Llama-3.1-8B-Instruct
  deepseek-ai/DeepSeek-R1-Distill-Llama-8B : deepseek-ai/DeepSeek-R1-Distill-Llama-8B

# Set the model parameters here
model_parameters:
  temperature: 0.7
  max_output_tokens: 10 # NOT USED

# Sampling rate determines how often a question is asked again if the answer format is wrong
max_sampling_rate: 1

# Set to 1 if you don't want to shuffle
num_of_shuffles: 5

number_of_questions: 46 # Not 100. TODO How does this change the analysis?

# Set the names for result/evaluation files here

# Turn on/off tracking of results and prints
track_results: true
print_results: true

# Dataset name
dataset_name: "mmlu_pro_Computer_Security_46"

# Type of prompt engineering
prompt_engineering: "5_Shot_COT"

# Date format
date_format: "%Y_%m_%d_%H_%M"

# Set output path
output_path: "./results/{number_of_questions}_questions_{dataset_name}/"

# Set output file name
output_evaluation: "{output_path}llm_{prompt_engineering}_{dataset_name}.pkl"

# Filename output evaluation detailed
output_evaluation_detailed: "./results/{number_of_questions}_questions_{prompt_engineering}_{dataset_name}/llm_prob_result_detailed_{dataset_name}_{prompt_engineering}.pkl"

# Set filename of json file
output_evaluation_json: "./results/{number_of_questions}_questions_{prompt_engineering}_{dataset_name}/llm_prob_result_{dataset_name}_{prompt_engineering}.json"

# Create folder for results if not exists
create_results_folder: true

# Set the questions bank here
questions_bank: "./data/mmlu_Computer_Security.parquet"

# Set the prompt template here
prompt_template: LLAMA31_MMLU_PRO_5_SHOT_COT
