{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:26:40.756296Z",
     "start_time": "2025-03-18T16:26:40.751294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import pickle\n",
    "import argparse\n",
    "import re\n",
    "import os"
   ],
   "id": "6adf86deb51f19a6",
   "outputs": [],
   "execution_count": 436
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:26:40.811016Z",
     "start_time": "2025-03-18T16:26:40.803350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def improved_extract_answer(answer):\n",
    "    \"\"\"Extracts the correct answers from the provided answer string.\n",
    "\n",
    "    Args:\n",
    "        answer: The answer string to extract the correct answers from.\n",
    "\n",
    "    Returns:\n",
    "        A list of correct answers.\n",
    "    \"\"\"\n",
    "    # Remove reasoning sections enclosed in <think> tags\n",
    "    answer = re.sub(r\"<think>.*?</think>\", \"\", answer, flags=re.DOTALL).strip()\n",
    "\n",
    "    # Special case 1: Extract all \"[X]\" patterns in the entire text\n",
    "    bracketed_letters = re.findall(r\"\\[([A-J])\\]\", answer)\n",
    "    if len(bracketed_letters) > 1:\n",
    "        return sorted(set(bracketed_letters))\n",
    "\n",
    "    # Special case 2: \"Answer :[A, B, C]\" format (comma-separated list in one bracket)\n",
    "    if re.search(r\"Answer\\s*:\\s*\\[\", answer, re.IGNORECASE):\n",
    "        brackets_with_commas = re.search(r\"\\[(.*?)\\]\", answer)\n",
    "        if brackets_with_commas:\n",
    "            content = brackets_with_commas.group(1)\n",
    "            letters = re.findall(r\"([A-J])\", content)\n",
    "            if letters:\n",
    "                return sorted(set(letters))\n",
    "\n",
    "    # Find the answer section using regular pattern\n",
    "    pattern = re.compile(\n",
    "        r\"(?:answer is|answer:)\\s*\"  # Match indicators like \"answer is\" or \"answer:\"\n",
    "        r\"(.+?)\"  # Capture everything after the indicator\n",
    "        r\"(?:\\.|\\:|\\n|$)\",  # Until a period, colon, newline, or end of string\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    match = pattern.search(answer)\n",
    "    if not match:\n",
    "        return []\n",
    "\n",
    "    answer_section = match.group(1).strip()\n",
    "\n",
    "    # Handle special case with 'not'\n",
    "    if \"not\" in answer_section.lower():\n",
    "        not_matches = re.findall(r\"not\\s+[\\[\\(\\*]?([A-J])[\\]\\)\\*]?\", answer_section, re.IGNORECASE)\n",
    "        all_matches = re.findall(r\"([A-J])\", answer_section)\n",
    "\n",
    "        # Filter out 'not' letters\n",
    "        return sorted(set([letter for letter in all_matches if letter not in not_matches]))\n",
    "\n",
    "    # Extract all letters directly\n",
    "    all_letters = re.findall(r\"([A-J])\", answer_section)\n",
    "\n",
    "    return sorted(set(all_letters))  # Remove duplicates and return sorted list\n",
    "\n"
   ],
   "id": "c1b1c507fd990ad",
   "outputs": [],
   "execution_count": 437
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:26:40.862925Z",
     "start_time": "2025-03-18T16:26:40.857128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def their_extract_answer(answer):\n",
    "    \"\"\"Extracts the correct answers from the provided answer string.\n",
    "\n",
    "    Args:\n",
    "        answer: The answer string to extract the correct answers from.\n",
    "\n",
    "    Returns:\n",
    "        A list of correct answers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cleaning the input by removing some non-relevant characters\n",
    "    answer_proc = re.sub(r'[\\s\\n.,]', '', answer)\n",
    "\n",
    "    # Define regex patterns for different cases\n",
    "    pattern_single_letters = re.compile(r'^[A-J]+$')\n",
    "    #pattern1 = re.compile(r\"answer is \\(?([A-J]+)\\)?\", re.IGNORECASE)\n",
    "    pattern1 = re.compile(r\"answer is \\[?([A-J]+)\\]?\", re.IGNORECASE)\n",
    "    pattern2 = re.compile(r'.*[aA]nswer:\\s*([A-J]+)', re.IGNORECASE)\n",
    "\n",
    "    if re.match(pattern_single_letters, answer_proc):\n",
    "        return list(answer_proc)\n",
    "    else:\n",
    "        # Find matches using the first regex pattern\n",
    "        #drop , from answer\n",
    "\n",
    "        match1 = pattern1.findall(answer)\n",
    "\n",
    "        # Find matches using the second regex pattern\n",
    "        match2 = pattern2.findall(answer)\n",
    "\n",
    "        # Combine results from both patterns\n",
    "        results = match1 + match2\n",
    "\n",
    "        # Flatten the list and remove duplicates\n",
    "        combined_results = []\n",
    "        for result in results:\n",
    "            combined_results.extend(list(result))\n",
    "\n",
    "        return list(set(combined_results))"
   ],
   "id": "e8e579cca0509289",
   "outputs": [],
   "execution_count": 438
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:26:40.972386Z",
     "start_time": "2025-03-18T16:26:40.965290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_function(function_to_test):\n",
    "    \"\"\"Test the improved_extract_answer function with various cases.\"\"\"\n",
    "    test_cases = [\n",
    "        # Previously correct cases\n",
    "        (\"The answer is ABCDE.\", ['A', 'B', 'C', 'D', 'E']),\n",
    "        (\"The best answer is C, E.\", ['C', 'E']),\n",
    "        (\"The best answer is [DE]. The access and distribution layers must be on the same device\", ['D', 'E']),\n",
    "        (\"Answer:[A, B, C]\", ['A', 'B', 'C']),\n",
    "        (\"The best answer is A, and not B.\", ['A']),\n",
    "        (\"Answer: A and B:\", ['A', 'B']),\n",
    "        (\"The best answer is *A*\", ['A']),\n",
    "        (\"The best answer is (A)\", ['A']),\n",
    "\n",
    "        # Previously incorrect cases\n",
    "        (\"The best answer is A, and B.\", ['A', 'B']),\n",
    "        (\"The best answer is (A), (B), and (C)\", ['A', 'B', 'C']),\n",
    "        (\"Answer: [A], [B], and [C]:\", ['A', 'B', 'C']),\n",
    "        (\"Answer: A, B, and C:\", ['A', 'B', 'C']),\n",
    "        (\"Answer: A, B, C:\", ['A', 'B', 'C']),\n",
    "\n",
    "        # Think case\n",
    "        (\"\"\"\n",
    "<think>\n",
    "Okay, so I have this question about the ip route command in Cisco IOS. I'm a bit new to this, but I'll try to work through it step by step. The question is asking which two statements are true about the command: ip route 172.16.3.0 255.255.255.0 192.168.2.4.\n",
    "So the true statements are A and E.\n",
    "</think>\n",
    "\n",
    "The best answer is A and E.\n",
    "\"\"\", ['A','E'])\n",
    "    ]\n",
    "\n",
    "    passed_all_tests = True\n",
    "    for input_text, expected_output in test_cases:\n",
    "        result = function_to_test(input_text)\n",
    "        print(f\"{input_text} : {result} {'✓' if result == expected_output else '✗'}\")\n",
    "        if result != expected_output:\n",
    "            passed_all_tests = False\n",
    "\n",
    "    if passed_all_tests:\n",
    "        print(\"All tests passed\")\n",
    "    else:\n",
    "        print(\"Some tests failed\")"
   ],
   "id": "45ac9f42df6f6c46",
   "outputs": [],
   "execution_count": 440
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:26:41.034939Z",
     "start_time": "2025-03-18T16:26:41.029663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Improved extracted answer function\")\n",
    "test_function(improved_extract_answer)"
   ],
   "id": "bcd4d35bed5b2794",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved extracted answer function\n",
      "The answer is ABCDE. : ['A', 'B', 'C', 'D', 'E'] ✓\n",
      "The best answer is C, E. : ['C', 'E'] ✓\n",
      "The best answer is [DE]. The access and distribution layers must be on the same device : ['D', 'E'] ✓\n",
      "Answer:[A, B, C] : ['A', 'B', 'C'] ✓\n",
      "The best answer is A, and not B. : ['A'] ✓\n",
      "Answer: A and B: : ['A', 'B'] ✓\n",
      "The best answer is *A* : ['A'] ✓\n",
      "The best answer is (A) : ['A'] ✓\n",
      "The best answer is A, and B. : ['A', 'B'] ✓\n",
      "The best answer is (A), (B), and (C) : ['A', 'B', 'C'] ✓\n",
      "Answer: [A], [B], and [C]: : ['A', 'B', 'C'] ✓\n",
      "Answer: A, B, and C: : ['A', 'B', 'C'] ✓\n",
      "Answer: A, B, C: : ['A', 'B', 'C'] ✓\n",
      "\n",
      "<think>\n",
      "Okay, so I have this question about the ip route command in Cisco IOS. I'm a bit new to this, but I'll try to work through it step by step. The question is asking which two statements are true about the command: ip route 172.16.3.0 255.255.255.0 192.168.2.4.\n",
      "So the true statements are A and E.\n",
      "</think>\n",
      "\n",
      "The best answer is A and E.\n",
      " : ['A', 'E'] ✓\n",
      "All tests passed\n"
     ]
    }
   ],
   "execution_count": 441
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:26:41.202637Z",
     "start_time": "2025-03-18T16:26:41.198755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Their extracted answer function\")\n",
    "test_function(their_extract_answer)"
   ],
   "id": "a70fff581ea255da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Their extracted answer function\n",
      "The answer is ABCDE. : ['D', 'C', 'E', 'B', 'A'] ✗\n",
      "The best answer is C, E. : ['C'] ✗\n",
      "The best answer is [DE]. The access and distribution layers must be on the same device : ['D', 'E'] ✓\n",
      "Answer:[A, B, C] : [] ✗\n",
      "The best answer is A, and not B. : ['A'] ✓\n",
      "Answer: A and B: : ['A'] ✗\n",
      "The best answer is *A* : [] ✗\n",
      "The best answer is (A) : [] ✗\n",
      "The best answer is A, and B. : ['A'] ✗\n",
      "The best answer is (A), (B), and (C) : [] ✗\n",
      "Answer: [A], [B], and [C]: : [] ✗\n",
      "Answer: A, B, and C: : ['A'] ✗\n",
      "Answer: A, B, C: : ['A'] ✗\n",
      "\n",
      "<think>\n",
      "Okay, so I have this question about the ip route command in Cisco IOS. I'm a bit new to this, but I'll try to work through it step by step. The question is asking which two statements are true about the command: ip route 172.16.3.0 255.255.255.0 192.168.2.4.\n",
      "So the true statements are A and E.\n",
      "</think>\n",
      "\n",
      "The best answer is A and E.\n",
      " : ['A'] ✗\n",
      "Some tests failed\n"
     ]
    }
   ],
   "execution_count": 443
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:26:41.323283Z",
     "start_time": "2025-03-18T16:26:41.320356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_value(value):\n",
    "    \"\"\"\n",
    "    Modify this function to define how you want to process the values.\n",
    "    \"\"\"\n",
    "    return value * 2  # Example: Modify as per your requirements\n",
    "\n",
    "def process_pkl(input_file, output_file, llm_answer_column, temperature, exam, prompt_engineering):\n",
    "    # Load the DataFrame\n",
    "    with open(input_file, \"rb\") as f:\n",
    "        df = pickle.load(f)\n",
    "\n",
    "    # Ensure input column exists before processing\n",
    "    if llm_answer_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{llm_answer_column}' not found in the DataFrame.\")\n",
    "\n",
    "    improved_extracted_answer_column = \"Improved_Extracted_Answer_Column\"\n",
    "    their_extracted_answer_column = \"Their_Extracted_Answer_Column\"\n",
    "\n",
    "    # Apply functions to the llm answer column and save results in the new columns\n",
    "    df[improved_extracted_answer_column] = df[llm_answer_column].apply(improved_extract_answer)\n",
    "    df[their_extracted_answer_column] = df[llm_answer_column].apply(their_extract_answer)\n",
    "\n",
    "    # Add new columns with constant values\n",
    "    df[\"Temperature\"] = temperature\n",
    "    df[\"Exam\"] = exam\n",
    "    df[\"Prompt_Engineering\"] = prompt_engineering\n",
    "    df[\"Differ\"] = df[their_extracted_answer_column] != df[improved_extracted_answer_column]\n",
    "\n",
    "    # Define columns to keep\n",
    "    selected_columns = [\"Exam\",\n",
    "                        \"QuestionIndex\",\n",
    "                        \"NumberOfChoices\",\n",
    "                        \"Model\",\n",
    "                        \"SamplingIndex\",\n",
    "                        \"Temperature\",\n",
    "                        \"Prompt_Engineering\",\n",
    "                        llm_answer_column,\n",
    "                        \"Exam_Answers\",\n",
    "                        improved_extracted_answer_column,\n",
    "                        their_extracted_answer_column,\n",
    "                        \"Differ\"]\n",
    "\n",
    "    # Select only the specified columns\n",
    "    df_selected = df[selected_columns]\n",
    "\n",
    "    # Save the modified DataFrame as a .pkl file\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(df_selected, f)\n",
    "\n",
    "    print(f\"Processed file saved as: {output_file}\")"
   ],
   "id": "8db911583c92e183",
   "outputs": [],
   "execution_count": 444
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:26:41.373249Z",
     "start_time": "2025-03-18T16:26:41.368621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def batch_process_pkl(input_dir, output_dir, llm_answer_column, temperature, exam, prompt_engineering):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get all .pkl files in the input directory\n",
    "    pkl_files = glob.glob(os.path.join(input_dir, \"*.pkl\"))\n",
    "\n",
    "    if not pkl_files:\n",
    "        print(f\"No .pkl files found in {input_dir}.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(pkl_files)} .pkl files in {input_dir}. Processing...\")\n",
    "\n",
    "    # Loop through each file and process it\n",
    "    for input_file in pkl_files:\n",
    "        # Generate output file path\n",
    "        filename = os.path.basename(input_file)\n",
    "        output_file = os.path.join(output_dir, f\"processed_{filename}\")\n",
    "\n",
    "        # Process and save\n",
    "        process_pkl(input_file, output_file, llm_answer_column, temperature, exam, prompt_engineering)"
   ],
   "id": "30326691339cb54b",
   "outputs": [],
   "execution_count": 445
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:27:05.598768Z",
     "start_time": "2025-03-18T16:26:41.424706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_process_pkl(\n",
    "    input_dir=\"./exam/201-301-ccna/0_shot/t00/\",\n",
    "    output_dir=\"./processed_results/\",\n",
    "    llm_answer_column=\"LLM_Answer\",\n",
    "    temperature=0.0,\n",
    "    exam=\"CCNA-201-301\",\n",
    "    prompt_engineering=\"0_shot\"\n",
    ")"
   ],
   "id": "a2f8251192d40550",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 .pkl files in ./exam/201-301-ccna/0_shot/t00/. Processing...\n",
      "Processed file saved as: ./processed_results/processed_100_questions_201-301-CCNA_deepseek-ai_DeepSeek-R1-Distill-Llama-8B_20250206_0853_shuffled_1.pkl\n",
      "Processed file saved as: ./processed_results/processed_100_questions_201-301-CCNA_meta-llama_Llama-3.1-8B-Instruct_20250211_1528_shuffled_4.pkl\n",
      "Processed file saved as: ./processed_results/processed_100_questions_201-301-CCNA_deepseek-ai_DeepSeek-R1-Distill-Llama-8B_20250206_1111_shuffled_4.pkl\n",
      "Processed file saved as: ./processed_results/processed_100_questions_201-301-CCNA_deepseek-ai_DeepSeek-R1-Distill-Llama-8B_20250206_1027_shuffled_3.pkl\n",
      "Processed file saved as: ./processed_results/processed_100_questions_201-301-CCNA_meta-llama_Llama-3.1-8B-Instruct_20250211_1525_shuffled_1.pkl\n",
      "Processed file saved as: ./processed_results/processed_100_questions_201-301-CCNA_deepseek-ai_DeepSeek-R1-Distill-Llama-8B_20250206_0938_shuffled_2.pkl\n",
      "Processed file saved as: ./processed_results/processed_100_questions_201-301-CCNA_deepseek-ai_DeepSeek-R1-Distill-Llama-8B_20250206_0810_shuffled_0.pkl\n",
      "Processed file saved as: ./processed_results/processed_100_questions_201-301-CCNA_meta-llama_Llama-3.1-8B-Instruct_20250211_1523_shuffled_0.pkl\n",
      "Processed file saved as: ./processed_results/processed_100_questions_201-301-CCNA_meta-llama_Llama-3.1-8B-Instruct_20250211_1527_shuffled_3.pkl\n",
      "Processed file saved as: ./processed_results/processed_100_questions_201-301-CCNA_meta-llama_Llama-3.1-8B-Instruct_20250211_1527_shuffled_2.pkl\n"
     ]
    }
   ],
   "execution_count": 446
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:27:05.611999Z",
     "start_time": "2025-03-18T16:27:05.609427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_pkl_to_parquet(folder_path):\n",
    "    \"\"\"\n",
    "    Converts all .pkl files in the specified folder to .parquet format.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Error: The folder '{folder_path}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    converted_files = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pkl\"):\n",
    "            pkl_path = os.path.join(folder_path, filename)\n",
    "            parquet_path = pkl_path.replace(\".pkl\", \".parquet\")\n",
    "\n",
    "            try:\n",
    "                # Load the pickle file\n",
    "                df = pd.read_pickle(pkl_path)\n",
    "\n",
    "                # Convert to Parquet\n",
    "                df.to_parquet(parquet_path, index=False)\n",
    "\n",
    "                print(f\"Converted: {filename} → {os.path.basename(parquet_path)}\")\n",
    "                converted_files += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to convert {filename}: {e}\")\n",
    "\n",
    "    if converted_files == 0:\n",
    "        print(\"No .pkl files found in the folder.\")\n",
    "    else:\n",
    "        print(f\"Conversion complete: {converted_files} file(s) converted.\")\n"
   ],
   "id": "e8c67127ba4a9ff9",
   "outputs": [],
   "execution_count": 447
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:27:05.688971Z",
     "start_time": "2025-03-18T16:27:05.661255Z"
    }
   },
   "cell_type": "code",
   "source": "convert_pkl_to_parquet(\"./processed_results/\")\n",
   "id": "d869ad93874c6bf2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted: processed_100_questions_201-301-CCNA_meta-llama_Llama-3.1-8B-Instruct_20250211_1527_shuffled_3.pkl → processed_100_questions_201-301-CCNA_meta-llama_Llama-3.1-8B-Instruct_20250211_1527_shuffled_3.parquet\n",
      "Converted: processed_100_questions_201-301-CCNA_deepseek-ai_DeepSeek-R1-Distill-Llama-8B_20250206_0938_shuffled_2.pkl → processed_100_questions_201-301-CCNA_deepseek-ai_DeepSeek-R1-Distill-Llama-8B_20250206_0938_shuffled_2.parquet\n",
      "Converted: processed_100_questions_201-301-CCNA_meta-llama_Llama-3.1-8B-Instruct_20250211_1523_shuffled_0.pkl → processed_100_questions_201-301-CCNA_meta-llama_Llama-3.1-8B-Instruct_20250211_1523_shuffled_0.parquet\n",
      "Converted: processed_100_questions_201-301-CCNA_meta-llama_Llama-3.1-8B-Instruct_20250211_1525_shuffled_1.pkl → processed_100_questions_201-301-CCNA_meta-llama_Llama-3.1-8B-Instruct_20250211_1525_shuffled_1.parquet\n",
      "Converted: processed_100_questions_201-301-CCNA_meta-llama_Llama-3.1-8B-Instruct_20250211_1528_shuffled_4.pkl → processed_100_questions_201-301-CCNA_meta-llama_Llama-3.1-8B-Instruct_20250211_1528_shuffled_4.parquet\n",
      "Converted: processed_100_questions_201-301-CCNA_meta-llama_Llama-3.1-8B-Instruct_20250211_1527_shuffled_2.pkl → processed_100_questions_201-301-CCNA_meta-llama_Llama-3.1-8B-Instruct_20250211_1527_shuffled_2.parquet\n",
      "Converted: processed_100_questions_201-301-CCNA_deepseek-ai_DeepSeek-R1-Distill-Llama-8B_20250206_0810_shuffled_0.pkl → processed_100_questions_201-301-CCNA_deepseek-ai_DeepSeek-R1-Distill-Llama-8B_20250206_0810_shuffled_0.parquet\n",
      "Converted: processed_100_questions_201-301-CCNA_deepseek-ai_DeepSeek-R1-Distill-Llama-8B_20250206_1111_shuffled_4.pkl → processed_100_questions_201-301-CCNA_deepseek-ai_DeepSeek-R1-Distill-Llama-8B_20250206_1111_shuffled_4.parquet\n",
      "Converted: processed_100_questions_201-301-CCNA_deepseek-ai_DeepSeek-R1-Distill-Llama-8B_20250206_0853_shuffled_1.pkl → processed_100_questions_201-301-CCNA_deepseek-ai_DeepSeek-R1-Distill-Llama-8B_20250206_0853_shuffled_1.parquet\n",
      "Converted: processed_100_questions_201-301-CCNA_deepseek-ai_DeepSeek-R1-Distill-Llama-8B_20250206_1027_shuffled_3.pkl → processed_100_questions_201-301-CCNA_deepseek-ai_DeepSeek-R1-Distill-Llama-8B_20250206_1027_shuffled_3.parquet\n",
      "Conversion complete: 10 file(s) converted.\n"
     ]
    }
   ],
   "execution_count": 448
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:27:05.712034Z",
     "start_time": "2025-03-18T16:27:05.708905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compare_answers(answer_llm, answer_exam):\n",
    "    \"\"\"Compares the extracted correct answers with the answers in answer_exam.\n",
    "\n",
    "    Keyword arguments:\n",
    "    answerLLM -- the list of answers extracted from the LLM answer\n",
    "    answer_exam -- list of answers from the exam\n",
    "    \"\"\"\n",
    "    # Convert answer_exam_list from letters to numbers\n",
    "    answerLLM = [ord(answer) - 65 for answer in answer_llm]\n",
    "\n",
    "    # Get number of correct answers in the exam\n",
    "    num_of_correct_exam_answers = len(answer_exam)\n",
    "\n",
    "    # Convert both lists to sets for efficient comparison\n",
    "    answer_LLM_set = set(answerLLM)\n",
    "    answer_exam_set = set(answer_exam)\n",
    "\n",
    "    # Calculate the count of matching answers\n",
    "    number_of_correct_llm_answers = len(answer_LLM_set.intersection(answer_exam_set))\n",
    "\n",
    "    #Calculate the number of incorrect answers\n",
    "    number_of_incorrect_llm_answers = len(answer_LLM_set.difference(answer_exam_set))\n",
    "\n",
    "    # Check if the number of answers given by the LLM is greater than the number of correct answers\n",
    "    too_many_answ_given = False\n",
    "    if len(answer_LLM_set) > num_of_correct_exam_answers:\n",
    "        too_many_answ_given = True\n",
    "\n",
    "    # Return a dictionary with the matching count and the number of correct answers\n",
    "    return number_of_correct_llm_answers, too_many_answ_given, number_of_incorrect_llm_answers"
   ],
   "id": "9d7d1643fb43932c",
   "outputs": [],
   "execution_count": 449
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:27:05.764008Z",
     "start_time": "2025-03-18T16:27:05.759180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluation_sampling(fn_extractor, full_llm_answer, exam_answers):\n",
    "    \"\"\"Analyse the answer given by the LLM and compare it with the exam answers.\n",
    "\n",
    "    Keyword arguments:\n",
    "    fn_extractor -- the function that extracts the answer(s) from the llm_answer\n",
    "    llm_answer -- the answer string given by the LLM\n",
    "    exam_Answers -- the list of answers from the exam\n",
    "    \"\"\"\n",
    "    num_of_correct_answer = len(exam_answers)\n",
    "\n",
    "    llm_answers = fn_extractor(full_llm_answer)\n",
    "    if llm_answers is not None:\n",
    "        num_of_correct_llm_Answers, too_many_answ, number_of_incorrect_llm_answers = compare_answers(llm_answers, exam_answers)\n",
    "        if num_of_correct_llm_Answers == num_of_correct_answer and too_many_answ == False:\n",
    "            answered_correctly = True\n",
    "        else:\n",
    "            answered_correctly = False\n",
    "        return num_of_correct_llm_Answers, llm_answers, too_many_answ, answered_correctly, number_of_incorrect_llm_answers\n",
    "    else:\n",
    "         return -1"
   ],
   "id": "337cb534d0eadd",
   "outputs": [],
   "execution_count": 450
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:36:12.088465Z",
     "start_time": "2025-03-18T16:36:12.085182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluation(llm_output_dataframe):\n",
    "\n",
    "    # Compute the number of total questions for each model\n",
    "    number_of_questions = llm_output_dataframe.groupby(['Model','Prompt_Engineering', 'Temperature'])['QuestionIndex'].count()\n",
    "\n",
    "    #Number of fully correct answers given by the LLM\n",
    "    correctly_answered = llm_output_dataframe.groupby(['Model', 'Prompt_Engineering', 'Temperature'])['Answered_Correctly'].sum()\n",
    "\n",
    "    #Number of incorrect answers given by the LLM\n",
    "    incorrectly_answered = number_of_questions - correctly_answered\n",
    "\n",
    "    #Amount of correct answers in the exam\n",
    "    amount_correct_exam_answers = llm_output_dataframe.groupby(['Model', 'Prompt_Engineering', 'Temperature'])['NumberOfCorrectExamAnswers'].sum()\n",
    "\n",
    "    #Amount of correct answers given by the LLM even if not fully correct\n",
    "    amount_correct_llm_answers = llm_output_dataframe.groupby(['Model', 'Prompt_Engineering', 'Temperature'])['NumberOfCorrectLLMAnswers'].sum()\n",
    "\n",
    "    # Calculate Partial Credits\n",
    "    llm_output_dataframe['Partial_Credit'] = llm_output_dataframe.apply(\n",
    "        lambda row: max(0, row['NumberOfCorrectLLMAnswers'] / row['NumberOfCorrectExamAnswers'] -\n",
    "                        (row['NumberOfIncorrectLLMAnswers'] /row['NumberOfCorrectExamAnswers'])), axis=1)\n",
    "\n",
    "    # Aggregate Partial Credit for each model\n",
    "    partial_credit_sum = llm_output_dataframe.groupby(['Model', 'Prompt_Engineering', 'Temperature'])['Partial_Credit'].sum()\n",
    "\n",
    "    #Calculation of Accuracy and Recall and f1 score\n",
    "    accuracy = correctly_answered / number_of_questions\n",
    "    accuracy_partial = partial_credit_sum / number_of_questions\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Number of Questions': number_of_questions,\n",
    "        'Correctly Answered': correctly_answered,\n",
    "        'Incorrectly Answered': incorrectly_answered,\n",
    "        'Accuracy': accuracy,\n",
    "        'Accuracy Partial': accuracy_partial,\n",
    "        'Total Partial Credit': partial_credit_sum\n",
    "    })\n",
    "\n",
    "    results_df = results_df.reset_index()\n",
    "\n",
    "    return results_df"
   ],
   "id": "a85e80c9501754c6",
   "outputs": [],
   "execution_count": 462
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T16:34:42.697890Z",
     "start_time": "2025-03-18T16:34:42.694966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_model_statistics(df):\n",
    "    \"\"\"\n",
    "    Calculates statistics for each model in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame containing evaluation metrics for different models.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: New DataFrame containing calculated statistics for each model.\n",
    "    \"\"\"\n",
    "    model_stats = []\n",
    "    for model, group_df in df.groupby('Model'):\n",
    "        model_stat = {\n",
    "            'Model': model,\n",
    "            'Accuracy Mean': group_df['Accuracy'].mean(),\n",
    "            'Accuracy Max': group_df['Accuracy'].max(),\n",
    "            'Accuracy Min': group_df['Accuracy'].min(),\n",
    "            'Accuracy STD': group_df['Accuracy'].std(),\n",
    "            'Accuracy Partial Mean': group_df['Accuracy Partial'].mean(),\n",
    "            'Accuracy Partial Max': group_df['Accuracy Partial'].max(),\n",
    "            'Accuracy Partial Min': group_df['Accuracy Partial'].min(),\n",
    "            'Accuracy Partial STD': group_df['Accuracy Partial'].std()\n",
    "        }\n",
    "        model_stats.append(model_stat)\n",
    "\n",
    "    return pd.DataFrame(model_stats)"
   ],
   "id": "9f3e753a04fdd0b7",
   "outputs": [],
   "execution_count": 460
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T17:01:10.549830Z",
     "start_time": "2025-03-18T17:01:10.540808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def produce_statistics_from_batch(input_dir = \"./processed_results/\"):\n",
    "    # Get all .pkl files in the input directory\n",
    "    pkl_files = glob.glob(os.path.join(input_dir, \"*.pkl\"))\n",
    "\n",
    "    if not pkl_files:\n",
    "        print(f\"No .pkl files found in {input_dir}.\")\n",
    "        # return\n",
    "\n",
    "    print(f\"Found {len(pkl_files)} .pkl files in {input_dir}. Processing...\")\n",
    "\n",
    "    #Create a dataframe with the size of NUM_OF_SHUFFLES which contains the dataframe llm_exam_result\n",
    "    shuffled_evaluation_df = pd.DataFrame(columns=['Number of Questions', 'Correctly Answered', 'Incorrectly Answered', 'Accuracy', 'Accuracy Partial'])\n",
    "\n",
    "    # pickle_path = \"./processed_results/processed_100_questions_201-301-CCNA_meta-llama_Llama-3.1-8B-Instruct_20250211_1523_shuffled_0.pkl\"\n",
    "    # pickles = [pickle_path,]\n",
    "\n",
    "    for pickle in pkl_files:\n",
    "        llm_exam_result = pd.DataFrame(columns = [\n",
    "            \"Model\",\n",
    "            \"Prompt_Engineering\",\n",
    "            \"Temperature\",\n",
    "            \"Exam\",\n",
    "            \"QuestionIndex\",\n",
    "            \"SamplingIndex\",\n",
    "            # \"Improved_Extracted_Answer_Column\",\n",
    "            # \"Their_Extracted_Answer_Column\",\n",
    "            # \"Differ\",\n",
    "            \"NumberOfChoices\",\n",
    "            \"NumberOfCorrectLLMAnswers\",\n",
    "            \"NumberOfIncorrectLLMAnswers\",\n",
    "            \"NumberOfCorrectExamAnswers\",\n",
    "            \"Ratio\",\n",
    "            \"LLM_Answer\",\n",
    "            \"Exam_Answers\",\n",
    "            \"Answered_Correctly\",\n",
    "            \"Too_Many_answers\"\n",
    "        ])\n",
    "        result_from_exam = pd.read_pickle(pickle)\n",
    "        for index_question, row in result_from_exam.iterrows():\n",
    "            num_of_correct_answer = len(row[\"Exam_Answers\"])\n",
    "            # num_of_choices = row[\"NumberOfChoices\"] # TODO: Do not need. Remove\n",
    "            # extracted_answer = row[\"Improved_Extracted_Answer\"]\n",
    "\n",
    "            num_of_correct_llm_answer, answerLLm, too_many_answers, answered_correctly, number_of_incorrect_llm_answers = evaluation_sampling(improved_extract_answer, row[\"LLM_Answer\"],row[\"Exam_Answers\"])\n",
    "\n",
    "            new_row = pd.DataFrame({\n",
    "                \"Model\": [row[\"Model\"]], # M\n",
    "                \"Prompt_Engineering\": [row[\"Prompt_Engineering\"]],\n",
    "                \"Temperature\": [row[\"Temperature\"]],\n",
    "                \"Exam\": [row[\"Exam\"]],\n",
    "                \"QuestionIndex\": [row[\"QuestionIndex\"]], # M\n",
    "                \"SamplingIndex\": [row[\"SamplingIndex\"]],\n",
    "                # \"Improved_Extracted_Answer\" : row[\"Improved_Extracted_Answer\"],\n",
    "                # \"Their_Extracted_Answer\": row[\"Their_Extracted_Answer\"],\n",
    "                # \"Differ\": row[\"Differ],\n",
    "                \"NumberOfChoices\": row[\"NumberOfChoices\"],\n",
    "                \"NumberOfIncorrectLLMAnswers\": number_of_incorrect_llm_answers, # M\n",
    "                \"NumberOfCorrectLLMAnswers\": [num_of_correct_llm_answer], # M\n",
    "                \"NumberOfCorrectExamAnswers\": [num_of_correct_answer], # M\n",
    "                \"Ratio\": [num_of_correct_llm_answer/num_of_correct_answer],\n",
    "                \"LLM_Answer\": [row[\"LLM_Answer\"]],\n",
    "                \"Exam_Answers\": [row[\"Exam_Answers\"]],\n",
    "                \"Answered_Correctly\" : [answered_correctly], # M\n",
    "                \"Too_Many_answers\": [too_many_answers]})\n",
    "\n",
    "            if llm_exam_result.empty:\n",
    "                llm_exam_result = new_row  # Directly assign instead of concatenating\n",
    "            else:\n",
    "                llm_exam_result = pd.concat([llm_exam_result, new_row], ignore_index=True)\n",
    "\n",
    "        evaluation_df = evaluation(llm_exam_result)\n",
    "        #Concat the evaluation dataframe to the complete dataframe\n",
    "\n",
    "        if shuffled_evaluation_df.empty:\n",
    "            shuffled_evaluation_df = evaluation_df  # Directly assign instead of concatenating\n",
    "        else:\n",
    "            shuffled_evaluation_df = pd.concat([shuffled_evaluation_df, evaluation_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    print(shuffled_evaluation_df)\n",
    "    model_statistics = calculate_model_statistics(shuffled_evaluation_df)\n",
    "    print(model_statistics)"
   ],
   "id": "7aaa830e62a476c2",
   "outputs": [],
   "execution_count": 470
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T17:01:22.868269Z",
     "start_time": "2025-03-18T17:01:22.294774Z"
    }
   },
   "cell_type": "code",
   "source": "produce_statistics_from_batch()",
   "id": "c61f795b561422e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 .pkl files in ./processed_results/. Processing...\n",
      "                                      Model Prompt_Engineering  Temperature  \\\n",
      "0          meta-llama/Llama-3.1-8B-Instruct             0_shot          0.0   \n",
      "1  deepseek-ai/DeepSeek-R1-Distill-Llama-8B             0_shot          0.0   \n",
      "2          meta-llama/Llama-3.1-8B-Instruct             0_shot          0.0   \n",
      "3          meta-llama/Llama-3.1-8B-Instruct             0_shot          0.0   \n",
      "4          meta-llama/Llama-3.1-8B-Instruct             0_shot          0.0   \n",
      "5          meta-llama/Llama-3.1-8B-Instruct             0_shot          0.0   \n",
      "6  deepseek-ai/DeepSeek-R1-Distill-Llama-8B             0_shot          0.0   \n",
      "7  deepseek-ai/DeepSeek-R1-Distill-Llama-8B             0_shot          0.0   \n",
      "8  deepseek-ai/DeepSeek-R1-Distill-Llama-8B             0_shot          0.0   \n",
      "9  deepseek-ai/DeepSeek-R1-Distill-Llama-8B             0_shot          0.0   \n",
      "\n",
      "   Number of Questions  Correctly Answered  Incorrectly Answered  Accuracy  \\\n",
      "0                  100                  58                    42      0.58   \n",
      "1                  100                  53                    47      0.53   \n",
      "2                  100                  61                    39      0.61   \n",
      "3                  100                  57                    43      0.57   \n",
      "4                  100                  55                    45      0.55   \n",
      "5                  100                  57                    43      0.57   \n",
      "6                  100                  55                    45      0.55   \n",
      "7                  100                  59                    41      0.59   \n",
      "8                  100                  55                    45      0.55   \n",
      "9                  100                  56                    44      0.56   \n",
      "\n",
      "   Accuracy Partial  Total Partial Credit  \n",
      "0             0.580                  58.0  \n",
      "1             0.540                  54.0  \n",
      "2             0.610                  61.0  \n",
      "3             0.570                  57.0  \n",
      "4             0.550                  55.0  \n",
      "5             0.570                  57.0  \n",
      "6             0.555                  55.5  \n",
      "7             0.595                  59.5  \n",
      "8             0.560                  56.0  \n",
      "9             0.570                  57.0  \n",
      "                                      Model  Accuracy Mean  Accuracy Max  \\\n",
      "0  deepseek-ai/DeepSeek-R1-Distill-Llama-8B          0.556          0.59   \n",
      "1          meta-llama/Llama-3.1-8B-Instruct          0.576          0.61   \n",
      "\n",
      "   Accuracy Min  Accuracy STD  Accuracy Partial Mean  Accuracy Partial Max  \\\n",
      "0          0.53      0.021909                  0.564                 0.595   \n",
      "1          0.55      0.021909                  0.576                 0.610   \n",
      "\n",
      "   Accuracy Partial Min  Accuracy Partial STD  \n",
      "0                  0.54              0.020433  \n",
      "1                  0.55              0.021909  \n"
     ]
    }
   ],
   "execution_count": 471
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
